<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>模式识别复习 | 杀杀人上上网</title><meta name="author" content="Lexie"><meta name="copyright" content="Lexie"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="模式识别复习一、绪论1.什么是模式识别 人类智能：感知、学习、思维、语言理解与对话、行为 人工智能研究内容：机器感知（模式识别）、机器学习、机器思维（问题求解）、自然语言处理、只能行为  模式识别：使计算机模仿人的感知能力，从感知数据中提取信息的过程。 ​                    非结构化数据—&gt;结构化知识  模式识别相关问题：数据预处理、模式分割、运动分析、模式描述与分类（特">
<meta property="og:type" content="article">
<meta property="og:title" content="模式识别复习">
<meta property="og:url" content="https://lexie1216.github.io/2022/03/01/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%A4%8D%E4%B9%A0%E6%96%87%E6%A1%A3/index.html">
<meta property="og:site_name" content="杀杀人上上网">
<meta property="og:description" content="模式识别复习一、绪论1.什么是模式识别 人类智能：感知、学习、思维、语言理解与对话、行为 人工智能研究内容：机器感知（模式识别）、机器学习、机器思维（问题求解）、自然语言处理、只能行为  模式识别：使计算机模仿人的感知能力，从感知数据中提取信息的过程。 ​                    非结构化数据—&gt;结构化知识  模式识别相关问题：数据预处理、模式分割、运动分析、模式描述与分类（特">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png">
<meta property="article:published_time" content="2022-02-28T16:00:00.000Z">
<meta property="article:modified_time" content="2023-01-25T11:29:55.929Z">
<meta property="article:author" content="Lexie">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://lexie1216.github.io/2022/03/01/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%A4%8D%E4%B9%A0%E6%96%87%E6%A1%A3/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '模式识别复习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-01-25 19:29:55'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = url => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      link.onload = () => resolve()
      link.onerror = () => reject()
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="杀杀人上上网"><span class="site-name">杀杀人上上网</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">模式识别复习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-02-28T16:00:00.000Z" title="发表于 2022-03-01 00:00:00">2022-03-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-01-25T11:29:55.929Z" title="更新于 2023-01-25 19:29:55">2023-01-25</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="模式识别复习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="模式识别复习"><a href="#模式识别复习" class="headerlink" title="模式识别复习"></a>模式识别复习</h1><h2 id="一、绪论"><a href="#一、绪论" class="headerlink" title="一、绪论"></a>一、绪论</h2><h3 id="1-什么是模式识别"><a href="#1-什么是模式识别" class="headerlink" title="1.什么是模式识别"></a>1.什么是模式识别</h3><ul>
<li><p>人类智能：<strong>感知</strong>、学习、思维、语言理解与对话、行为</p>
<p>人工智能研究内容：<strong>机器感知（模式识别）</strong>、机器学习、机器思维（问题求解）、自然语言处理、只能行为</p>
</li>
<li><p>模式识别：使计算机模仿人的感知能力，从感知数据中提取信息的过程。</p>
<p>​                    非结构化数据—&gt;结构化知识</p>
</li>
<li><p>模式识别相关问题：数据预处理、模式分割、运动分析、<strong>模式描述与分类（特征提取&#x2F;选择、模式分类、聚类、机器学习）</strong>、模式识别应用研究</p>
</li>
</ul>
<h3 id="2-模式识别发展简史"><a href="#2-模式识别发展简史" class="headerlink" title="2.模式识别发展简史"></a>2.模式识别发展简史</h3><ul>
<li>80年代：多层神经网络，BP算法；卷积神经网络</li>
<li>90年代：SVM；多分类器系统（Ensemble）；半监督学习；多标签学习；多任务学习</li>
<li>21世纪初：概率图模型（马尔可夫随机场、隐马尔可夫模型、条件随机场）；迁移学习；深度学习</li>
<li>ML&#x2F;PR&#x2F;DM主要方法：分类&#x2F;聚类&#x2F;特征提取<ul>
<li>ML：从数据、经验中获取知识、规则、模型、参数的过程，主要研究通用理论算法，大部分针对分类</li>
<li>DM：针对各种数据中的信息提取和知识发现</li>
<li>PR：主要研究分类识别方法，面向感知应用</li>
<li>CV：模仿人类视觉系统，实现视觉信息高层理解</li>
</ul>
</li>
</ul>
<h3 id="3-模式识别形式化"><a href="#3-模式识别形式化" class="headerlink" title="3.模式识别形式化"></a>3.模式识别形式化</h3><ul>
<li>模式的两个层次：样本&amp;类别</li>
<li>模式识别核心技术：模式分类</li>
<li>模式的计算机表示：<ul>
<li>识别对象的表示（特征）：特征矢量、特征空间</li>
<li>分类器表示：类别模型、判别函数、决策面</li>
</ul>
</li>
</ul>
<h3 id="4-模式识别系统流程"><a href="#4-模式识别系统流程" class="headerlink" title="4.模式识别系统流程"></a>4.模式识别系统流程</h3><p>训练&#x2F;测试过程中的模式预处理和特征提取必须完全一致</p>
<h3 id="5-模式分类器设计"><a href="#5-模式分类器设计" class="headerlink" title="5.模式分类器设计"></a>5.模式分类器设计</h3><ul>
<li>数据划分的两个层次<ul>
<li>性能评价：Training+Test</li>
<li>模型选择：Estimation+Validation</li>
</ul>
</li>
<li>泛化性能（Bias-Variance Tradeoff图）</li>
</ul>
<h3 id="6-模式识别方法分类"><a href="#6-模式识别方法分类" class="headerlink" title="6.模式识别方法分类"></a>6.模式识别方法分类</h3><ul>
<li>按模式&#x2F;模型表示方法分类<ul>
<li>统计方法：需要大量数据训练，解释性差，与人类认知的相关性低</li>
<li>结构方法：小样本情况下性能良好，大样本训练困难，对outlier鲁棒</li>
</ul>
</li>
<li>学习方法分类<ul>
<li>监督学习</li>
<li>无监督学习</li>
<li>半监督学习</li>
<li>强化学习</li>
<li>领域自适应（迁移学习）：测试样本分布发生变化，分类器参数自适应</li>
<li>增量学习、在线学习：数据顺序出现（且过去数据不能保存），学新不忘旧</li>
</ul>
</li>
<li>生成&#x2F;判别（都属于统计模式识别方法？）<ul>
<li>生成模型Generative：表示各个类别内部结构或特征分布，不同类别分别学习。</li>
<li>判别模型Discriminative：表示不同类别之间的区别，一般为判别函数、边界函数或后验概率。所有类别样本同时学习</li>
<li>混合模型的几种方式<ul>
<li>生成模型+判别学习</li>
<li>混合模型  $f_1(x,\theta_1)+f_2(x,\theta_2)$</li>
<li>混合学习准则 $l_1(x,\theta_1)+l_2(x,\theta_2)$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="二、贝叶斯决策-生成模型"><a href="#二、贝叶斯决策-生成模型" class="headerlink" title="二、贝叶斯决策(生成模型)"></a>二、贝叶斯决策(生成模型)</h2><h3 id="1-分类问题的表示"><a href="#1-分类问题的表示" class="headerlink" title="1.分类问题的表示"></a>1.分类问题的表示</h3><ul>
<li>类别：        $\omega_i,i&#x3D;1,…,c$</li>
<li>特征矢量： $x&#x3D;[x_1,…,x_d] \in R^d$</li>
<li>先验概率： $P(\omega_i)\quad\textstyle\sum_{i&#x3D;1}^c P(\omega_i)&#x3D;1$</li>
<li>概率密度函数（条件概率）：$p(x|\omega_i)$</li>
<li>后验概率：$P(\omega_i|x)&#x3D;\frac{p(x|\omega_i)P(\omega_i)}{p(x)}&#x3D;\frac{p(x|\omega_i)P(\omega_i)}{\sum^{c}<em>{j&#x3D;1}p(x|\omega_j)P(\omega_j)} \quad \sum^c</em>{i&#x3D;1}P(w_i|x)&#x3D;1$</li>
</ul>
<h3 id="3-最小风险决策（贝叶斯决策的一般形式）"><a href="#3-最小风险决策（贝叶斯决策的一般形式）" class="headerlink" title="3.最小风险决策（贝叶斯决策的一般形式）"></a>3.最小风险决策（贝叶斯决策的一般形式）</h3><ul>
<li><p>风险函数 $\lambda(\alpha_i|\omega_j)$ 描述类别状态为$\omega_j$ 时采取行动$\alpha_i$ 的风险</p>
</li>
<li><p>条件风险$R(\alpha_i|x)&#x3D;\sum_{j&#x3D;1}^c \lambda(\alpha_i|\omega_j)P(\omega_j|x)$ </p>
</li>
<li><p>为了最小化总风险$R&#x3D;\int R(\alpha(x)|x)p(x)dx$ ，</p>
<p>对所有$i&#x3D;1,…,a$ 计算条件风险$R(\alpha_i|x)&#x3D;\sum_{j&#x3D;1}^c \lambda(\alpha_i|\omega_j)P(\omega_j|x)$ ，</p>
<p>并且选择行为$\alpha_i $使$R(\alpha_i|x)$ 最小化。</p>
</li>
<li><p>两类分类问题</p>
<ul>
<li><p>简化 $\lambda_{ij}&#x3D;\lambda(\alpha_i|\omega_j)$ 表示当实际类别为$\omega_j$ 却误判为$\omega_i$ 所引起的损失。</p>
<p>$R(\alpha_1|x)&#x3D;\lambda_{11}P(\omega_1|x)+\lambda_{12}P(\omega_2|x)$</p>
<p>$R(\alpha_2|x)&#x3D;\lambda_{21}P(\omega_1|x)+\lambda_{22}P(\omega_2|x)$</p>
</li>
<li><p>最小风险决策基本规则（表示1）：</p>
<p>如果$R(\alpha_1|x)&lt;R(\alpha_2|x)$ ，则判为$\omega_1$</p>
</li>
<li><p>用后验概率形式表述为（表示2）：</p>
<p>如果$(\lambda_{21}-\lambda_{11})P(\omega_1|x)&gt;(\lambda_{12}-\lambda_{22})P(\omega_2|x)$ ，则判为$\omega_1$</p>
</li>
<li><p>利用贝叶斯公式，用先验概率和条件密度来表示后验概率，等价规则为（表示3）：</p>
<p>如果$(\lambda_{21}-\lambda_{11})p(x|\omega_1)P(\omega_1)&gt;(\lambda_{12}-\lambda_{22})p(x|\omega_2)P(\omega_2)$ ，则判为$\omega_1$</p>
</li>
<li><p>通常，一次错误判决所造成的损失比正确判决要大，所以可以合理假设$\lambda_{21}-\lambda_{11}$ 和$\lambda_{12}-\lambda_{22}$ 都是正的，等价规则为（表示4）：</p>
<p>如果$\frac{p(x|\omega_1)}{p(x|\omega_2)}&gt;\frac{\lambda_{12}-\lambda_{22}}{\lambda_{21}-\lambda_{11}} \frac{P(\omega_2)}{P(\omega_1}$  ，则判为$\omega_1$</p>
<p>考虑$p(x|\omega_j)$ 作为$\omega_j $的函数（似然函数），等式左边的式子构成似然比。因此贝叶斯决策规则可以解释成如果似然比超过某个不依赖观测值$x$的阈值，那么可判决为$\omega_1$。</p>
</li>
</ul>
</li>
</ul>
<h3 id="3-最小误差率分类（最大后验概率决策）"><a href="#3-最小误差率分类（最大后验概率决策）" class="headerlink" title="3.最小误差率分类（最大后验概率决策）"></a>3.最小误差率分类（最大后验概率决策）</h3><ul>
<li>损失函数（“对称损失”或“0-1损失”函数）：</li>
</ul>
<p>$$<br>\lambda(\alpha_i|\omega_j)&#x3D;<br>\begin{cases}<br>0, \quad i&#x3D;j \<br>1, \quad i\neq j<br>\end{cases}<br>\qquad i,j&#x3D;1,…,c<br>$$</p>
<ul>
<li>这个损失函数对应的风险（平均误差概率）：</li>
</ul>
<p>$$<br>R(\alpha_i|x) &amp;&#x3D;\sum_{j&#x3D;1}^c \lambda_{ij}P(\omega_j|x)<br>\ &amp;&#x3D;\sum_{j \neq i} P(\omega_j|x)<br>\ &amp;&#x3D; 1-  P(\omega_i|x)<br>$$</p>
<ul>
<li><p>最小化平均误差概率条件下的贝叶斯决策规则：（后验概率最大）</p>
<p>对任给$j \neq i$， 如果$P(\omega_i|x)\gt P(\omega_j|x)$  ，则判决为$\omega_i $</p>
</li>
</ul>
<h3 id="3-5-带拒识的决策（c-1-classes"><a href="#3-5-带拒识的决策（c-1-classes" class="headerlink" title="3.5. 带拒识的决策（c+1 classes)"></a>3.5. 带拒识的决策（c+1 classes)</h3><ul>
<li><p>损失函数</p>
<p>$$<br>\lambda(\alpha_i|\omega_j)&#x3D;<br>\begin{cases}<br>0,                     &amp; i&#x3D;j \<br>\lambda_s,    &amp; i\neq j \<br>\lambda_r,     &amp; reject<br>\end{cases}<br>$$</p>
</li>
<li><p>风险函数</p>
<p>$$<br>R(\alpha_i|x) &#x3D;\sum_{j&#x3D;1}^c \lambda_{ij}P(\omega_j|x)<br>$$</p>
<p>$$<br>R_i(x)&#x3D;<br>\begin{cases}<br>\lambda_s[1-P(\omega_i|x)],&amp;i&#x3D;1,…,c    \<br>\lambda_r,&amp;reject<br>\end{cases}<br>$$</p>
</li>
<li><h6 id="决策规则"><a href="#决策规则" class="headerlink" title="决策规则"></a>决策规则</h6><p>$$<br>\arg\min_i R_i(x)&#x3D;<br>\begin{cases}<br>\arg\max_i P(\omega_i|x), &amp;if \max_i P(\omega_i|x) \gt 1-\lambda_r&#x2F;\lambda_s \<br>reject, &amp;otherwise<br>\end{cases}<br>$$</p>
<p> 最大后验概率小于阈值$1-\lambda_r&#x2F;\lambda_s$时，拒识。</p>
</li>
</ul>
<h3 id="4-判别函数和决策面"><a href="#4-判别函数和决策面" class="headerlink" title="4.判别函数和决策面"></a>4.判别函数和决策面</h3><ul>
<li><p>判别函数：</p>
<p>表征模式属于每一类的广义似然度$g_i(x),i&#x3D;1,…,c$</p>
<p>分类决策 $\arg\max_i g_i(x)$</p>
<p>例如：</p>
<ul>
<li>条件风险：$g_i(x)&#x3D;-R(\alpha_i|x)$</li>
<li>后验概率：$g_i(x)&#x3D;P(\omega_i|x)$</li>
<li>似然函数：$g_i(x)&#x3D;\log p (x|\omega_i)+\log P(\omega_i)$</li>
</ul>
</li>
<li><p>决策面：特征空间中两类判别函数相等的点的集合</p>
</li>
</ul>
<h3 id="5-0-贝叶斯决策用于模式分类"><a href="#5-0-贝叶斯决策用于模式分类" class="headerlink" title="5.0. 贝叶斯决策用于模式分类"></a>5.0. 贝叶斯决策用于模式分类</h3><ul>
<li>贝叶斯决策的关键：<ul>
<li>类条件概率密度估计</li>
<li>先验概率：从训练样本估计或假设等概率</li>
<li>决策代价$[\lambda_{ij}]$ ，一般为0-1代价</li>
</ul>
</li>
</ul>
<h3 id="5-高斯概率密度"><a href="#5-高斯概率密度" class="headerlink" title="5.高斯概率密度"></a>5.高斯概率密度</h3><ul>
<li>单变量高斯密度函数（正态分布）</li>
</ul>
<p>$$<br>p(x)&#x3D;\frac{1}{\sqrt{2\pi}\sigma}\exp[-\frac{1}{2}(\frac{x-\mu}{\sigma})^2]<br>\<br>\mu \equiv \varepsilon[x] &#x3D; \int_{-\infty}^{\infty}  xp(x)dx<br>\<br>\sigma^2  \equiv \varepsilon[(x-\mu)^2]&#x3D; \int_{-\infty}^{\infty} (x-\mu)^2p(x)dx<br>$$</p>
<ul>
<li>多元密度函数</li>
</ul>
<p>$$<br>p(x)&#x3D;\frac{1}{(2\pi)^{d&#x2F;2}\abs{\Sigma}^{1&#x2F;2}}\exp[-\frac{1}{2}(x-\mu)^t\Sigma^{-1}(x-\mu)]<br>\<br>\mu \equiv \varepsilon[x] &#x3D; \int xp(x)dx \quad \mu_i&#x3D;\varepsilon[x_i]<br>\<br>\Sigma  \equiv \varepsilon[(x-\mu)(x-\mu)^t]&#x3D; \int (x-\mu)(x-\mu)^t p(x)dx \quad \<br>\sigma_{ij}&#x3D;\varepsilon[(x_i-\mu_i)(x_j-\mu_j)] \quad 若x_i与x_j独立，则\sigma_{ij}&#x3D;0<br>$$</p>
<ul>
<li><p>协方差矩阵$\Sigma$ 的性质</p>
<ul>
<li><p>实对称</p>
</li>
<li><p>特征值分解，特征向量构成的矩阵$\Phi$是正交的</p>
</li>
<li><p>可对角化 $\Phi^T \Sigma \Phi &#x3D;\Lambda$</p>
</li>
<li><p>应用：PCA</p>
</li>
<li><p>服从正态分布的随机变量的线性组合（独立或非独立的都可以）也是一个正态分布。定义白化变换$A_w&#x3D;\Phi\Lambda^{-t1&#x2F;2}$ ，可以产生一个圆轴对称的高斯分布。</p>
<p>$$<br>\begin{align*}<br>A^t_w\Sigma A_w&amp;&#x3D;\Lambda^{-1&#x2F;2}\Phi^t\Sigma\Phi\Lambda^{-1&#x2F;2}\<br>&amp;&#x3D;\Lambda^{-1&#x2F;2}\Lambda\Lambda^{1&#x2F;2}\<br>&amp;&#x3D;I<br>\end{align*}<br>$$</p>
</li>
</ul>
</li>
</ul>
<h3 id="6-高斯密度下的判别函数（LDF-amp-QDF）"><a href="#6-高斯密度下的判别函数（LDF-amp-QDF）" class="headerlink" title="6.高斯密度下的判别函数（LDF&amp;QDF）"></a>6.高斯密度下的判别函数（LDF&amp;QDF）</h3><ul>
<li><p>二、4中由类似然函数相等得到的判别函数：$g_i(x)&#x3D;\log p (x|\omega_i)+\log P(\omega_i)$ </p>
<p>如果密度函数$p(x|\omega_i)$ 是多元正态分布，则可得到判别函数：</p>
</li>
</ul>
<p>$$<br>g_i(x)&#x3D;-\frac{1}{2}(x-\mu_i)^t\Sigma_i^{-1}(x-\mu_i)-\frac{d}{2}\ln{2\pi}-\frac{1}{2}\ln{\abs{\Sigma_i}}+\ln{P(\omega_i)}<br>$$</p>
<ul>
<li><p>特殊情况讨论：</p>
<ul>
<li><p>$\Sigma_i&#x3D;\sigma^2I$ </p>
<ul>
<li><p>各特征统计独立，且每个特征具有相同的$\sigma^2$</p>
</li>
<li><p>等价的线性判别函数：</p>
<p>$$<br>g_i(x)&#x3D;w_i^tx+w_{i0}\<br>w_i&#x3D;\frac{1}{\sigma^2}\mu_i\<br>w_{i0}&#x3D;-\frac{1}{2\sigma^2}\mu_i^t\mu_i+\ln{P(\omega_i)}<br>$$</p>
</li>
<li><p>决策面：</p>
<p>$$</p>
<p>$$</p>
<p>$$<br>x_0&#x3D;\frac{1}{2}(\mu_i+\mu_j)-\frac{\sigma^2}{\norm{\mu_i-\mu_j}^2}\ln{\frac{P(\omega_i)}{P(\omega_j)}}(\mu_i-\mu_j)<br>$$</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>$\Sigma_i&#x3D;\Sigma$    <strong>（LDF）</strong></p>
<ul>
<li><p>所有类的协方差矩阵都相等，但各自的均值向量是任意的</p>
</li>
<li><p>等价的线性判别函数：</p>
<p>$$<br>g_i(x)&#x3D;w_i^tx+w_{i0}\<br>w_i&#x3D;\Sigma^{-1}\mu_i\<br>w_{i0}&#x3D;-\frac{1}{2}\mu_i^t\Sigma^{-1}\mu_i+\ln{P(\omega_i)}<br>$$</p>
<ul>
<li><p>决策面：</p>
<p>$$<br>x_0&#x3D;\frac{1}{2}(\mu_i+\mu_j)-\frac{1}{(\mu_i-\mu_j)^t\Sigma^{-1}(\mu_i-\mu_j)}\ln{\frac{P(\omega_i)}{P(\omega_j)}}(\mu_i-\mu_j)<br>$$</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p>$\Sigma_i&#x3D;任意$ <strong>（QDF）</strong></p>
<ul>
<li><p>每一类的协方差矩阵是不同的</p>
</li>
<li><p>等价的线性判别函数：</p>
<p>$$<br>g_i(x)&#x3D;x^tW_ix+w_i^tx+w_{i0}\<br>W_i&#x3D;-\frac{1}{2}\Sigma_i^{-1}\<br>w_i&#x3D;\Sigma_i^{-1}\mu_i\<br>w_{i0}&#x3D;-\frac{1}{2}\mu_i^t\Sigma_i^{-1}\mu_i-\frac{1}{2}\ln{\abs{\Sigma_i}}+\ln{P(\omega_i)}<br>$$</p>
</li>
</ul>
</li>
</ul>
<h3 id="7-分类错误率"><a href="#7-分类错误率" class="headerlink" title="7.分类错误率"></a>7.分类错误率</h3><ul>
<li>2类的情况</li>
</ul>
<p>$$<br>\begin{align*}<br>P(error)&amp;&#x3D;P(x \in \mathcal{R}_2,\omega_1)+P(x \in \mathcal{R}_1,\omega_2)\<br>&amp;&#x3D;P(x \in \mathcal{R}_2|\omega_1)P(\omega_1)+P(x \in \mathcal{R}<em>1|\omega_2)P(\omega_2)\<br>&amp;&#x3D;\int\limits</em>{\mathcal{R}<em>2}p(x|\omega_1)P(\omega_1)dx+\int\limits</em>{\mathcal{R}_1}p(x|\omega_2)P(\omega_2)dx<br>\end{align*}<br>$$</p>
<ul>
<li><p>一般情况</p>
<p>$$<br>\begin{align*}<br>P(correct)&amp;&#x3D;\sum_{i&#x3D;1}^c P(x\in \mathcal{R}<em>i,\omega_i)\<br>&amp;&#x3D;\sum</em>{i&#x3D;1}^cP(x\in\mathcal{R}<em>i|\omega_i)P(\omega_i)\<br>&amp;&#x3D;\sum</em>{i&#x3D;1}^c\int\limits_{\mathcal{R}_i}p(x|\omega_i)P(\omega_i)dx<br>\end{align*}<br>$$</p>
</li>
<li><p>最大后验概率决策（MAP）的情况</p>
<p>$$<br>\begin{align*}<br>P(correct)&amp;&#x3D;\int\limits_x \max_i P(x|\omega_i)P(\omega_i)dx\<br>&amp;&#x3D;\int\limits_x \max_i P(\omega_i|x)P(x)dx\<br>\end{align*}<br>$$</p>
<p>$$<br>P(error)&#x3D;\int\limits_x[1-\max_i P(\omega_i|x)]P(x)dx<br>$$</p>
</li>
</ul>
<h3 id="8-离散变量的贝叶斯决策"><a href="#8-离散变量的贝叶斯决策" class="headerlink" title="8.离散变量的贝叶斯决策"></a>8.离散变量的贝叶斯决策</h3><h3 id="9-复合模式分类"><a href="#9-复合模式分类" class="headerlink" title="9.复合模式分类"></a>9.复合模式分类</h3><h3 id="10-往年试题"><a href="#10-往年试题" class="headerlink" title="10.往年试题"></a>10.往年试题</h3><ul>
<li>2018年<ul>
<li>贝叶斯最小风险决策和最小错误率决策的决策规则（6分）</li>
<li>d维空间，c类分类，每一类条件概率密度为高斯分布<ul>
<li>写出最小错误率决策的判别函数，并说明在什么条件下判别函数为线性判别函数（5分）</li>
<li>c&#x3D;2，高斯密度条件下线性判别的决策面函数，说明类先验概率如何影响决策面的位置，并说明在什么情况下决策面与两个类中心差向量$\mu_1-\mu_2$ 垂直（举例说明两种情况即可）（6分）</li>
</ul>
</li>
</ul>
</li>
<li>2017年<ul>
<li>贝叶斯最小风险决策和最小错误率决策的决策规则（5分）</li>
<li>引入拒识，最小损失决策的决策规则（5分）</li>
<li>2维空间，2类，概率密度为高斯分布，给出均值、协方差和先验概率<ul>
<li>分类误差最小的贝叶斯决策的决策面函数，并写出贝叶斯错误率（积分形式）（6分）</li>
<li>当$\lambda_{11}&#x3D;\lambda_{22}&#x3D;0,\lambda_{12}&#x3D;2\lambda_{21}$ ，请给出损失最小的贝叶斯决策的决策面函数（4分）</li>
</ul>
</li>
</ul>
</li>
<li>2016年<ul>
<li>贝叶斯最小风险决策和最小错误率决策的决策规则（8分）</li>
<li>d维空间，c类分类，假设各类先验概率相等，每一类条件概率密度为高斯分布<ul>
<li>类协方差矩阵不等和相等两种情况下的最小错误率决策判别函数（6分）</li>
<li>c&#x3D;2，$P(\omega_1)&#x3D;P(\omega_2)$ ，两类概率密度均为高斯分布且协方差矩阵相等，写出贝叶斯分类决策面和贝叶斯错误率的公式（6分）</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="三、参数估计"><a href="#三、参数估计" class="headerlink" title="三、参数估计"></a>三、参数估计</h2><ul>
<li><p>给定分类器结构&#x2F;函数形式，从训练样本估计参数</p>
</li>
<li><p>统计生成模型的参数估计（概率密度）</p>
<ul>
<li><p>1.最大似然估计</p>
<p>假设参数为确定值，最优估计：似然度最大MLE</p>
</li>
<li><p>2.贝叶斯估计</p>
<p>假设参数为随机变量，估计其后验分布MAP</p>
</li>
</ul>
</li>
<li><p>统计判别模型的参数估计（判别函数）</p>
</li>
</ul>
<h3 id="1-最大似然估计"><a href="#1-最大似然估计" class="headerlink" title="1.最大似然估计"></a>1.最大似然估计</h3><ul>
<li><p>假设概率密度函数$p(x|\omega_i,\theta_i)$， 估计$\theta_i$</p>
</li>
<li><p>样本数据$D_1,…,D_c$</p>
<ul>
<li>$D_i$ 中的样本独立同分布</li>
<li>$D_i$ 用来估计$\theta_i$</li>
</ul>
</li>
<li><p>估计一类模型的参数</p>
<ul>
<li><p>似然函数：$p(\mathcal{D}|\theta)&#x3D;\prod_{k&#x3D;1}^np(x_k|\theta)$</p>
</li>
<li><p>最大化似然函数：$\max_\theta p(D|\theta)\leftrightarrow \gradient_\theta p(D|\theta)&#x3D;0$</p>
</li>
<li><p>p维参数空间上的梯度向量：</p>
<p>$$<br>\gradient_\theta \equiv<br>\begin{bmatrix}<br>\frac{\partial}{\partial\theta_1}        \<br>\vdots\<br>\frac{\partial}{\partial\theta_p}<br>\end{bmatrix}<br>$$</p>
</li>
<li><p>$\gradient_\theta p(D|\theta)&#x3D;0$ 的解可能有解析解，也可能需要迭代求解（如梯度下降）</p>
</li>
<li><p>对数似然函数：$l(\theta)\equiv\ln{p(\mathcal{D}|\theta)} \quad l(\theta)&#x3D;\sum_{k&#x3D;1}^n\ln{p(x_k|\theta)}$</p>
<p>最大似然估计：$\hat{\theta}&#x3D;\arg\max_\theta l(\theta)$</p>
<p>​                            $\gradient_\theta l &#x3D;\sum_{k&#x3D;1}^n\gradient_\theta\ln{p(x_k|\theta)}&#x3D;0$</p>
<p>​                            $\frac{\partial l}{\partial \theta_j}&#x3D;0,\quad j&#x3D;1,..,p$</p>
</li>
<li><p>讨论当训练样本服从多元正态分布时的情况</p>
<ul>
<li><p>$\mu$ 未知</p>
<p>$\hat{\mu}&#x3D;\frac{1}{n}\sum_{k&#x3D;1}^n x_k$</p>
</li>
<li><p>$\mu$ 和$\Sigma$ 均未知</p>
<p>$\hat{\mu}&#x3D;\frac{1}{n}\sum_{k&#x3D;1}^n x_k$<br>$\hat{\Sigma}&#x3D;\frac{1}{n}\sum_{k&#x3D;1}^n(x_k-\hat{\mu})(x_k-\hat{\mu})^t$</p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-贝叶斯估计"><a href="#2-贝叶斯估计" class="headerlink" title="2.贝叶斯估计"></a>2.贝叶斯估计</h3><h3 id="2-5最大似然估计和贝叶斯估计对比"><a href="#2-5最大似然估计和贝叶斯估计对比" class="headerlink" title="*2.5最大似然估计和贝叶斯估计对比"></a>*2.5最大似然估计和贝叶斯估计对比</h3><ul>
<li>当训练样本量趋近于无穷时，ML和BL的效果是一样的</li>
<li>计算复杂度：<ul>
<li>ML计算简单，只涉及一些微分运算或梯度搜索技术</li>
<li>BL可能要求计算非常复杂的多重积分</li>
</ul>
</li>
<li>可理解性：<ul>
<li>ML易理解，得到的结果是基于样本的最佳解答</li>
<li>BL难于直观理解，得到的结果是许多可行解答的加权平均值，反映出对各种可行解答的不确定程度</li>
</ul>
</li>
<li>对初始先验知识的信任程度：<ul>
<li>ML得到的结果$p(x|\hat{\theta})$ 的形式与初始假设的形式一致</li>
<li>BL得到的结果$p(x|\hat{\theta})$ 的形式与初始假设的形式不一定相同</li>
<li>通过使用全部$p(\theta|\mathcal{D})$ 中的信息，BL比ML能够利用更多有用的信息</li>
<li>如果没有特别的先验知识（$\theta$ 均匀分布），BL和ML是相似的。</li>
</ul>
</li>
<li>样本点很少的情况下，ML的效果并不好</li>
<li>ML估计的是$\theta$ 空间中的一个点，而BL估计的则是一个概率分布</li>
</ul>
<h3 id="3-特征维数问题"><a href="#3-特征维数问题" class="headerlink" title="3.特征维数问题"></a>3.特征维数问题</h3><h3 id="4-期望最大法"><a href="#4-期望最大法" class="headerlink" title="4.期望最大法"></a>4.期望最大法</h3><p>数据缺失情况下的参数估计</p>
<h3 id="5-隐马尔可夫模型"><a href="#5-隐马尔可夫模型" class="headerlink" title="5.隐马尔可夫模型"></a>5.隐马尔可夫模型</h3><h3 id="6-往年试题"><a href="#6-往年试题" class="headerlink" title="6.往年试题"></a>6.往年试题</h3><ul>
<li>2018年<ul>
<li>2维空间，2个类别分别4个样本，两类都服从高斯分布，求最大似然估计参数值。进一步假设两个类别先验概率相等，写出分类决策面公式（6分）</li>
</ul>
</li>
<li>2016年<ul>
<li>2维空间，2个类别分别4个样本，两类都服从高斯分布，求最大似然估计参数值。进一步假设两个类别先验概率相等，写出分类决策面公式（8分）</li>
</ul>
</li>
</ul>
<h2 id="四、非参数方法"><a href="#四、非参数方法" class="headerlink" title="四、非参数方法"></a>四、非参数方法</h2><h3 id="1-密度估计"><a href="#1-密度估计" class="headerlink" title="1.密度估计"></a>1.密度估计</h3><h3 id="2-Parzen窗方法"><a href="#2-Parzen窗方法" class="headerlink" title="2.Parzen窗方法"></a>2.Parzen窗方法</h3><h3 id="2-5-Parzen窗估计和k-近邻估计的区别"><a href="#2-5-Parzen窗估计和k-近邻估计的区别" class="headerlink" title="*2.5.Parzen窗估计和k-近邻估计的区别"></a>*2.5.Parzen窗估计和k-近邻估计的区别</h3><h3 id="3-K近邻估计"><a href="#3-K近邻估计" class="headerlink" title="3.K近邻估计"></a>3.K近邻估计</h3><h3 id="4-最近邻规则"><a href="#4-最近邻规则" class="headerlink" title="4.最近邻规则"></a>4.最近邻规则</h3><h3 id="5-距离度量"><a href="#5-距离度量" class="headerlink" title="5.距离度量"></a>5.距离度量</h3><h3 id="6-Approximation-by-Series-Expansion"><a href="#6-Approximation-by-Series-Expansion" class="headerlink" title="6.Approximation by Series Expansion"></a>6.Approximation by Series Expansion</h3><h3 id="7-往年试题"><a href="#7-往年试题" class="headerlink" title="7.往年试题"></a>7.往年试题</h3><ul>
<li>2018年<ul>
<li>说明Parzen窗估计和k-近邻估计的区别（4分）</li>
<li>给定2维空间三个样本点，写出概率密度函数$p(x)$ 的最近邻(1-NN)估计密度公式（这种情况下V为圆形面积）（4分）</li>
<li>对于c个类别，基于k-NN密度估计进行贝叶斯分类，写出各个类别的后验概率并证明（4分）</li>
</ul>
</li>
<li>2017年<ul>
<li>说明Parzen窗估计和k-近邻估计的区别（5分）</li>
<li>已知$\varphi(u)$ 写出概率密度函数的Parzen窗估计$p_n(x)$ （5分）</li>
<li>给定2维空间三个样本点，写出概率密度函数$p(x)$ 的最近邻(1-NN)估计并画出概率密度函数曲线图（5分）</li>
</ul>
</li>
<li>2016年<ul>
<li>说明Parzen窗估计和k-近邻估计的区别（5分）</li>
<li>对于c个类别，基于k-NN密度估计进行贝叶斯分类，写出各个类别的后验概率并证明（5分）</li>
</ul>
</li>
</ul>
<h2 id="五、线性判别函数"><a href="#五、线性判别函数" class="headerlink" title="五、线性判别函数"></a>五、线性判别函数</h2><ul>
<li><p>假定用于分类的判别函数的形式已知，直接从样本来估计判别函数的参数</p>
</li>
<li><p>模式分类的途径：</p>
<ul>
<li>估计类条件概率密度函数：<ul>
<li>利用贝叶斯公式求出后验概率</li>
<li>核心步骤：概率密度估计（参数估计和非参数估计）</li>
</ul>
</li>
<li>直接估计后验概率（K近邻）</li>
<li>直接计算判别函数</li>
</ul>
</li>
<li><p>利用样本直接设计分类器的方法分类：</p>
<ul>
<li>线性判别函数、SVM、Fisher线性判别函数</li>
<li>广义线性判别函数、非线性判别函数、核学习机</li>
</ul>
</li>
</ul>
<h3 id="1-线性判别函数与决策面"><a href="#1-线性判别函数与决策面" class="headerlink" title="1.线性判别函数与决策面"></a>1.线性判别函数与决策面</h3><ul>
<li><p>基本形式：$g(x)&#x3D;w^Tx+w_0$ </p>
</li>
<li><p>两类情形的决策规则：</p>
<p>$$<br>\begin{cases}<br>x \in \omega_1,                     &amp; if\quad  g(x) \gt 0\<br>x \in \omega_2,                     &amp; if \quad g(x) \lt 0\<br>uncertain,                     &amp; if\quad g(x) &#x3D; 0<br>\end{cases}<br>$$</p>
</li>
<li><p>两类情形的决策面：</p>
<p>超平面$H：g(x)&#x3D;0$ ，位于该平面的任意向量与$w$ 垂直。</p>
</li>
<li><p>多类情形：</p>
</li>
</ul>
<h3 id="2-广义线性判别函数"><a href="#2-广义线性判别函数" class="headerlink" title="2.广义线性判别函数"></a>2.广义线性判别函数</h3><h3 id="3-感知准则函数"><a href="#3-感知准则函数" class="headerlink" title="3.感知准则函数"></a>3.感知准则函数</h3><h3 id="4-松弛方法"><a href="#4-松弛方法" class="headerlink" title="4.松弛方法"></a>4.松弛方法</h3><h3 id="5-最小平方误差（MSE）准则函数"><a href="#5-最小平方误差（MSE）准则函数" class="headerlink" title="5.最小平方误差（MSE）准则函数"></a>5.最小平方误差（MSE）准则函数</h3><h3 id="6-Ho-Kashyap方法"><a href="#6-Ho-Kashyap方法" class="headerlink" title="6.Ho-Kashyap方法"></a>6.Ho-Kashyap方法</h3><h3 id="7-往年试题-1"><a href="#7-往年试题-1" class="headerlink" title="7.往年试题"></a>7.往年试题</h3><ul>
<li>2018年<ul>
<li>简述感知器（感知准则函数）算法的基本思想，并给出一种感知器学习算法（5分）</li>
</ul>
</li>
<li>2017年<ul>
<li>one-vs-all技巧，3类，3个判别函数，画出分类决策面（6分）</li>
<li>简述感知器（感知准则函数）算法的基本思想，并给出一种感知器学习算法（5分）</li>
</ul>
</li>
<li>2016年<ul>
<li>四个样本，两个类别，二维空间，批处理感知器算法求权向量（10分）</li>
</ul>
</li>
</ul>
<h2 id="六、神经网络"><a href="#六、神经网络" class="headerlink" title="六、神经网络"></a>六、神经网络</h2><h3 id="1-人工神经元"><a href="#1-人工神经元" class="headerlink" title="1.人工神经元"></a>1.人工神经元</h3><h3 id="2-拓扑结构"><a href="#2-拓扑结构" class="headerlink" title="2.拓扑结构"></a>2.拓扑结构</h3><h3 id="3-网络训练"><a href="#3-网络训练" class="headerlink" title="3.网络训练"></a>3.网络训练</h3><ul>
<li>Hebb训练方法（经验方法）：两结点的连接权重按它们的输出值之积来改变</li>
<li>$\delta$ 训练方法（分析方法）：梯度下降法</li>
<li>随机训练方法：随机改变一个权重，计算改变后产生的最终能量，并按如下准则来确定是否接受此改变，模拟退火算法</li>
<li>Kohonen训练方法（无监督）：<strong>自组织</strong>竞争型神经网络</li>
</ul>
<h3 id="4-单层前馈神经网络（单层感知机）"><a href="#4-单层前馈神经网络（单层感知机）" class="headerlink" title="4.单层前馈神经网络（单层感知机）"></a>4.单层前馈神经网络（单层感知机）</h3><p>单样本随机更新算法：适用于超大规模数据</p>
<p>批量更新算法：所有样本完成之后再更新</p>
<h3 id="5-多层感知器"><a href="#5-多层感知器" class="headerlink" title="5.多层感知器"></a>5.多层感知器</h3><h3 id="6-BP算法"><a href="#6-BP算法" class="headerlink" title="6.BP算法"></a>6.BP算法</h3><p>经典问题：</p>
<ul>
<li><p>完全难以训练</p>
<ul>
<li><p>网络的麻痹现象：$f’(net)\rightarrow 0$ 时，$\delta \rightarrow 0$ ，从而$\delta w_{ij} \rightarrow 0$ ，相当于调节过程几乎停顿下来。梯度更新将在S型函数的饱和区域进行，即处在其导数$f’(net)$ 非常小的区域内(平坦区域)。</p>
<p>![image-20220102170556841](&#x2F;Users&#x2F;Lexie&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20220102170556841.png)</p>
</li>
<li><p>梯度消失</p>
<img src="/Users/Lexie/Library/Application Support/typora-user-images/image-20220102170608637.png" alt="image-20220102170608637" style="zoom:33%;" />
</li>
<li><p>局部最小</p>
<img src="/Users/Lexie/Library/Application Support/typora-user-images/image-20220102170650804.png" alt="image-20220102170650804" style="zoom:30%;" /></li>
</ul>
</li>
<li><p>训练时间过长</p>
<ul>
<li>复杂问题需要很长时间训练</li>
<li>可能选取了不恰当的训练速率$\eta$</li>
</ul>
</li>
</ul>
<h3 id="7-径向基函数网络"><a href="#7-径向基函数网络" class="headerlink" title="7.径向基函数网络"></a>7.径向基函数网络</h3><h3 id="8-反馈神经网络"><a href="#8-反馈神经网络" class="headerlink" title="8.反馈神经网络"></a>8.反馈神经网络</h3><h3 id="9-自组织映射"><a href="#9-自组织映射" class="headerlink" title="9.自组织映射"></a>9.自组织映射</h3><h3 id="10-卷积神经网络CNN"><a href="#10-卷积神经网络CNN" class="headerlink" title="10.卷积神经网络CNN"></a>10.卷积神经网络CNN</h3><h3 id="11-自编码器"><a href="#11-自编码器" class="headerlink" title="11.自编码器"></a>11.自编码器</h3><h3 id="12-Recurrent-NN"><a href="#12-Recurrent-NN" class="headerlink" title="12.Recurrent NN"></a>12.Recurrent NN</h3><p>权重是共享的，因此简单地采用现有BP方法显得效率不高。</p>
<p>Back Propagation Through Time（BPTT）算法</p>
<img src="/Users/Lexie/Library/Application Support/typora-user-images/image-20220102181448271.png" alt="image-20220102181448271" style="zoom:33%;" />

<img src="/Users/Lexie/Library/Application Support/typora-user-images/image-20220102181509343.png" alt="image-20220102181509343" style="zoom:33%;" />

<h3 id="13-LSTM"><a href="#13-LSTM" class="headerlink" title="13.LSTM"></a>13.LSTM</h3><h3 id="14-往年试题"><a href="#14-往年试题" class="headerlink" title="14.往年试题"></a>14.往年试题</h3><ul>
<li><p>2018（15分）</p>
<p>d维空间，n个样本，c个类别，三层前向神经网络（输入层、隐含层、输出层），平方损失函数作为目标函数，写出权重$w_{ih}$ 和权重$w_{hj}$ 的更新公式，并简明扼要地给出其推导过程。</p>
</li>
<li><p>2017（15分）</p>
<p>d维空间，n个样本，c个类别，三层前向神经网络（输入层、隐含层、输出层），交叉熵损失函数作为目标函数，推导误差反向传播算法，并写出具体的推导过程。</p>
</li>
<li><p>2016（15分）</p>
<ul>
<li>针对多层前馈神经网络，请给出误差反向传播算法（即BP算法）的原理；结合三层网络给出有关权重更新的公式，并用文字描述所述公式的含义。（9）</li>
<li>请描述自组织映射网络的构造原理；针对网络训练，请给出自组织算法的主要计算步骤。（6）</li>
</ul>
</li>
</ul>
<h2 id="七、特征提取与选择"><a href="#七、特征提取与选择" class="headerlink" title="七、特征提取与选择"></a>七、特征提取与选择</h2><p>线性特征变换通常维度更低：PCA、LDA、ICA</p>
<p>非线性特征变换通常性能更好：KPCA、KLDA、Isomap、LLE、HLLE、LSTA</p>
<h3 id="1-特征提取"><a href="#1-特征提取" class="headerlink" title="1.特征提取"></a>1.特征提取</h3><p>特征提取的最终形式都是使用向量来表示数据样本，便于分析</p>
<ul>
<li>语音特征提取<ul>
<li>技术路线：预处理、分帧、加窗处理、特定数学运算得到低维向量作为提取的特征</li>
<li>MFCCs特征（Mel Frequency Cepstral Coefficients梅尔倒谱系数）</li>
</ul>
</li>
<li>文本特征提取<ul>
<li>词频-逆向文档频率（TF-IDF）</li>
<li>Word2Vec（主要模型：连续词袋模型CBOW，跳字模型Skip Gram）</li>
</ul>
</li>
<li>视觉特征提取<ul>
<li>局部二值模式（LBP）：应用于人脸识别</li>
<li>Gabor特征提取</li>
<li>尺度不变特征变化（SIFT）：<ul>
<li>局部方法：对图像中的局部区域进行分析</li>
<li>特征点检测+特征点描述</li>
<li>技术路线：高斯尺度空间构建GSS，高斯差分尺度空间构建DoGSS，极值点检测，特征点精细定位，特征点主方向计算，特征描述子生成</li>
<li>基于SIFT的图像匹配算法</li>
</ul>
</li>
<li>视觉词袋（Bag of Visual Words）<ul>
<li>技术路线：提取特征，学习视觉词汇，用视觉词汇量化特征，用视觉单词的频率表达图像</li>
</ul>
</li>
<li>哈尔特征（Haar）：应用于人脸识别<ul>
<li>一个Haar特征由一组方形滤波器组成，响应值为白色滤波器相应值-灰色滤波器相应值</li>
<li>通过比较Haar特征值是否超过某个阈值来判断是否为人脸，需要集成大量haar特征的判定结果来判断（Adaboost）</li>
<li>积分图快速计算</li>
</ul>
</li>
<li>梯度方向直方图（HoG）：最早用于行人检测，后来发展成面向一般物体检测<ul>
<li>技术路线：预处理（颜色转换、Gamma校正），计算图像梯度信息，划分图像区域成若干个cell，统计每个cell的梯度方向直方图，将相邻区域内的cell组成block串联起来进行归一化得到block特征，将block特征串联起来组成图像HoG特征</li>
<li>特征维度比较高，通常结合线性SVM进行分类</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-特征变换"><a href="#2-特征变换" class="headerlink" title="2.特征变换"></a>2.特征变换</h3><ul>
<li><p>线性降维</p>
<p>不同方法的差异：对低维子空间的性质有不同的要求，即对变换矩阵$W \in R^{d\times m}$施加不同的约束</p>
<ul>
<li>PCA<ul>
<li>目标：寻找一组方差较大的方向，将样本在该方向进行投影</li>
<li>求解过程：</li>
<li>算法步骤：</li>
</ul>
</li>
<li>LDA<ul>
<li>目标：寻找一组投影方向，使样本在投影之后类内样本点尽可能靠近，类间样本点尽可能互相远离，提升样本表示的分类鉴别能力</li>
<li>求解过程：</li>
<li>算法步骤：</li>
<li>局部线性判别分析（Neighborhood constraints&#x2F;Locally weighting）</li>
</ul>
</li>
<li>多维缩放MDS<ul>
<li>目标：降维后的样本仍保持两两之间的距离</li>
</ul>
</li>
<li>其他维数缩减方法<ul>
<li>经典方法：<strong>独立成分分析ICA</strong>，<strong>典型关联分析CCA</strong>，2DPCA，2DLAD，KPCA</li>
<li>流形学习方法：LLE，Isomap，LE，LTSA</li>
<li>深度学习方法：PCANet，RBM，DBN，DBM，AutoEncoder，Deep CCA</li>
</ul>
</li>
</ul>
</li>
<li><p>流形学习</p>
<ul>
<li><p>在数学上，流形用于描述一个几何体，他在<strong>局部</strong>具有欧式空间的性质</p>
</li>
<li><p>基本思想：高维空间相似的数据点映射到低维空间距离也是相似的</p>
</li>
<li><p>方法：对于给定数据集，通过最近邻等方式构造一个数据图。然后在每一个局部区域，高维空间中的某种相似度&#x2F;距离在低维空间中得以保持</p>
</li>
<li><p>几乎所有的流形学习方法都需要首先构建一个关于数据的图</p>
</li>
<li><p>经典算法：</p>
<ul>
<li><p>LLE局部线性嵌入：保持样本线性重构关系</p>
<p>最优线性表示系数：</p>
<p>全局嵌入学习模型：</p>
<p><strong>计算流程：1.线性表示，2.保持表示，3.全局误差，4.低维嵌入</strong></p>
<p>算法步骤：</p>
</li>
<li><p>Isomap等距映射：保持样本对之间的测地距离</p>
<p>算法步骤：</p>
</li>
<li><p>LE拉普拉斯特征映射：保持样本对之间的亲合度</p>
<p>学习模型：</p>
<p>算法步骤：</p>
</li>
<li><p>LTSA局部切空间对齐</p>
</li>
<li><p>LSE局部样条嵌入</p>
</li>
<li><p>LPP局部保持投影</p>
<p>学习模型：</p>
</li>
</ul>
</li>
<li><p>统一的学习模型：</p>
<ul>
<li>目标：通过非线性变换寻找给定高维数据的低维表示</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="3-特征选择"><a href="#3-特征选择" class="headerlink" title="3.特征选择"></a>3.特征选择</h3><ul>
<li><p>最优特征选择方法</p>
<ul>
<li><p>穷举法</p>
</li>
<li><p>分支定界法</p>
</li>
</ul>
</li>
<li><p>特征选择的次优方法（贪心策略）</p>
<ul>
<li>过滤式特征选择方法：“选择”与“学习”独立<ul>
<li>单独特征选择法</li>
<li>顺序前进特征选择法</li>
<li>顺序后退特征选择法</li>
<li>增l减r特征选择法</li>
<li>启发式选择方法：Relief方法</li>
</ul>
</li>
<li>包裹式特征选择方法：“选择”依赖“学习”</li>
<li>嵌入式特征选择方法：“选择”与“学习”同时进行</li>
</ul>
</li>
</ul>
<h3 id="4-往年试题"><a href="#4-往年试题" class="headerlink" title="4.往年试题"></a>4.往年试题</h3><ul>
<li><p>2018（8分）</p>
<ul>
<li>简述并比较PCA、CCA、LDA、ICA的区别和适用场景</li>
<li>详细阐述一种实现非线性数据降维的方式</li>
</ul>
</li>
<li><p>2017（12分）</p>
<ul>
<li><p>简述PCA（主成分分析）的主要思想及其求解过程</p>
</li>
<li><p>比较PCA、CCA、LDA、ICA的区别和适用场景</p>
</li>
<li><p>解释LDA（线性判别分析）所基于的数据分布假设，并阐述其不足之处</p>
</li>
</ul>
</li>
<li><p>2016（12分）</p>
<ul>
<li>简述LDA（线性判别分析）的主要思想</li>
<li>基于上述思想，给出两类问题的LDA目标函数</li>
<li>最优化上述目标函数，得到LDA结果</li>
</ul>
</li>
</ul>
<h2 id="八、模型选择"><a href="#八、模型选择" class="headerlink" title="八、模型选择"></a>八、模型选择</h2><h3 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h3><ul>
<li><p>机器学习</p>
<ul>
<li><p>学习&#x3D;表示+评价+优化</p>
<ul>
<li><p>表示：为学习器选择一种表示就以为选择一个特定的分类器集合。该集合被称为学习器的假设空间</p>
</li>
<li><p><img src="/Users/Lexie/Desktop/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%A1%A8%E7%A4%BA.png"></p>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-模型选择原则"><a href="#2-模型选择原则" class="headerlink" title="2.模型选择原则"></a>2.模型选择原则</h3><h3 id="3-模型评价标准"><a href="#3-模型评价标准" class="headerlink" title="3.模型评价标准"></a>3.模型评价标准</h3><ul>
<li>训练样本的划分<ul>
<li>刀切法（留一法）：每次从样本集中删除一个或者多个样本</li>
<li>自助法（Bootstrap）：每次有放回地随机抽取n个样本</li>
<li>保持方法（Holdout）：一部分用于训练，一部分用于<strong>测试</strong></li>
<li><strong>交叉验证（cross-validation）</strong>：将数据平分为k个子集，用k-1个子集进行训练，余下的部分用于<strong>验证</strong>，并计算验证误差。重复这一过程k次，得到k次结果的平均。（常用于模型参数选择）</li>
</ul>
</li>
</ul>
<h3 id="4-分类器集成"><a href="#4-分类器集成" class="headerlink" title="4.分类器集成"></a>4.分类器集成</h3><ul>
<li><p>集成学习的有效条件：</p>
<ul>
<li>每个单一的学习器错误率都应当低于0.5</li>
<li>进行集成学习的每个分类器应当各不相同</li>
</ul>
</li>
<li><p>集成学习的常用技术手段：</p>
<ul>
<li><strong>通过处理训练数据（bagging，boosting）</strong>：对训练样本进行随机分组，对错分样本进行加权</li>
<li>通过处理特征：每次只选择一部分特征来训练</li>
<li>通过处理类别标号：对多类问题，一对一策略、一对多策略</li>
<li>通过改进学习方法：变更学习参数、模型结构</li>
</ul>
</li>
<li><p>算法分类<strong>（按训练数据处理方式）</strong></p>
<ul>
<li><p>Bagging</p>
</li>
<li><p>Random subspace</p>
</li>
<li><p>Boosting&#x2F;adaboost</p>
</li>
<li><p>随机森林</p>
</li>
</ul>
</li>
</ul>
<h3 id="5-层叠泛化Stacked-Generalization"><a href="#5-层叠泛化Stacked-Generalization" class="headerlink" title="5.层叠泛化Stacked Generalization"></a>5.层叠泛化Stacked Generalization</h3><ul>
<li>采用多层结构，第一层的学习器配置不同的学习算法$L_1,L_2,..,L_T$，第一层的输出作为第二层的输入，<strong>第二层学习层称为元学习器L</strong></li>
</ul>
<h3 id="6-样本装袋Bagging"><a href="#6-样本装袋Bagging" class="headerlink" title="6.样本装袋Bagging"></a>6.样本装袋Bagging</h3><ul>
<li>训练一组基分类器，每个基分类器通过一个bootstrap训练样本集（通过有放回地随机抽样）来训练</li>
<li>获得基本分类器之后，通过投票进行统计</li>
</ul>
<h3 id="7-随机子空间Random-Subspace"><a href="#7-随机子空间Random-Subspace" class="headerlink" title="7.随机子空间Random Subspace"></a>7.随机子空间Random Subspace</h3><ul>
<li>通常也被称为<strong>属性装袋</strong>（attribute bagging）</li>
<li>基分类器通常由线性分类器、支持向量机等组成</li>
</ul>
<h3 id="8-Adaboost"><a href="#8-Adaboost" class="headerlink" title="8.Adaboost"></a>8.Adaboost</h3><h3 id="9-对Adaboost的理论解释"><a href="#9-对Adaboost的理论解释" class="headerlink" title="9.对Adaboost的理论解释"></a>9.对Adaboost的理论解释</h3><h3 id="10-基于Adaboost的人脸检测"><a href="#10-基于Adaboost的人脸检测" class="headerlink" title="10.基于Adaboost的人脸检测"></a>10.基于Adaboost的人脸检测</h3><h3 id="11-往年试题"><a href="#11-往年试题" class="headerlink" title="11.往年试题"></a>11.往年试题</h3><ul>
<li><p>2018年</p>
<p>针对两类分类问题简述Adaboost算法的基本计算过程（5分）</p>
</li>
</ul>
<h2 id="九、聚类"><a href="#九、聚类" class="headerlink" title="九、聚类"></a>九、聚类</h2><h3 id="1-引言-1"><a href="#1-引言-1" class="headerlink" title="1.引言"></a>1.引言</h3><h3 id="2-距离与相似性度量"><a href="#2-距离与相似性度量" class="headerlink" title="2.距离与相似性度量"></a>2.距离与相似性度量</h3><h3 id="3-K-means"><a href="#3-K-means" class="headerlink" title="3.K-means"></a>3.K-means</h3><h3 id="4-Gaussian-Mixture-Models"><a href="#4-Gaussian-Mixture-Models" class="headerlink" title="4.Gaussian Mixture Models"></a>4.Gaussian Mixture Models</h3><h3 id="5-Hierachical-Clustring"><a href="#5-Hierachical-Clustring" class="headerlink" title="5.Hierachical Clustring"></a>5.Hierachical Clustring</h3><h3 id="6-Spectral-Clustering"><a href="#6-Spectral-Clustering" class="headerlink" title="6.Spectral Clustering"></a>6.Spectral Clustering</h3><h3 id="7-Kernel-Clustering"><a href="#7-Kernel-Clustering" class="headerlink" title="7.Kernel Clustering"></a>7.Kernel Clustering</h3><h3 id="8-Deep-Clustering"><a href="#8-Deep-Clustering" class="headerlink" title="8.Deep Clustering"></a>8.Deep Clustering</h3><h3 id="9-往年试题"><a href="#9-往年试题" class="headerlink" title="9.往年试题"></a>9.往年试题</h3><ul>
<li><p>2018年</p>
<ul>
<li><p>简述谱聚类算法的基本思想，并指出可能影响谱聚类性能的因素（5分）</p>
</li>
<li><p>按最小距离准则对六个样本进行分级聚类，并画出聚类系统树图（10分）</p>
</li>
</ul>
</li>
<li><p>2017年</p>
<ul>
<li>从混合高斯密度函数估计的角度，简述K-Means聚类算法的原理（4分）</li>
<li>按最小距离准则对六个样本进行分级聚类，并画出聚类系统树图（8分）</li>
</ul>
</li>
<li><p>2016年</p>
<ul>
<li>K-menas对八个二维空间样本聚类的计算过程和结果（6分）</li>
<li>对GMM进行参数估计的过程中，哪些条件下可以导出K-means聚类算法（4分）</li>
</ul>
</li>
</ul>
<h2 id="十、支持向量机与核方法"><a href="#十、支持向量机与核方法" class="headerlink" title="十、支持向量机与核方法"></a>十、支持向量机与核方法</h2><h3 id="1-结构风险最小化"><a href="#1-结构风险最小化" class="headerlink" title="1.结构风险最小化"></a>1.结构风险最小化</h3><h3 id="2-VC维"><a href="#2-VC维" class="headerlink" title="2.VC维"></a>2.VC维</h3><h3 id="3-Hard-Margin-SVM"><a href="#3-Hard-Margin-SVM" class="headerlink" title="3.Hard-Margin SVM"></a>3.Hard-Margin SVM</h3><h3 id="4-Soft-Margin-SVM"><a href="#4-Soft-Margin-SVM" class="headerlink" title="4.Soft-Margin SVM"></a>4.Soft-Margin SVM</h3><h3 id="5-Dual-Problem"><a href="#5-Dual-Problem" class="headerlink" title="5.Dual Problem"></a>5.Dual Problem</h3><h3 id="6-Kernel-Methods"><a href="#6-Kernel-Methods" class="headerlink" title="6.Kernel Methods"></a>6.Kernel Methods</h3><h3 id="7-模型选择"><a href="#7-模型选择" class="headerlink" title="7.模型选择"></a>7.模型选择</h3><img src="/Users/Lexie/Library/Application Support/typora-user-images/image-20211231090348534.png" alt="image-20211231090348534" style="zoom:50%;" />



<h3 id="8-往年试题"><a href="#8-往年试题" class="headerlink" title="8.往年试题"></a>8.往年试题</h3><ul>
<li><h2 id="2018年"><a href="#2018年" class="headerlink" title="2018年"></a>2018年</h2></li>
<li><h2 id="2017年"><a href="#2017年" class="headerlink" title="2017年"></a>2017年</h2></li>
<li><h2 id="2016年"><a href="#2016年" class="headerlink" title="2016年"></a>2016年</h2></li>
</ul>
<h2 id="十一、决策树方法"><a href="#十一、决策树方法" class="headerlink" title="十一、决策树方法"></a>十一、决策树方法</h2><h3 id="1-信息增益算法"><a href="#1-信息增益算法" class="headerlink" title="1.信息增益算法"></a>1.信息增益算法</h3><h3 id="2-ID3"><a href="#2-ID3" class="headerlink" title="2.ID3"></a>2.ID3</h3><h3 id="3-C4-5"><a href="#3-C4-5" class="headerlink" title="3.C4.5"></a>3.C4.5</h3><h3 id="4-CART"><a href="#4-CART" class="headerlink" title="4.CART"></a>4.CART</h3><h3 id="5-过拟合"><a href="#5-过拟合" class="headerlink" title="5.过拟合"></a>5.过拟合</h3><h3 id="6-随机森林"><a href="#6-随机森林" class="headerlink" title="6.随机森林"></a>6.随机森林</h3><h3 id="7-往年试题-2"><a href="#7-往年试题-2" class="headerlink" title="7.往年试题"></a>7.往年试题</h3><ul>
<li><p>2018年</p>
<ul>
<li>描述ID3、C4.5、CART三种决策树方法的区别（4分）</li>
<li>阐述随机森林（Random Forests）的核心思想（4分）</li>
</ul>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://lexie1216.github.io">Lexie</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://lexie1216.github.io/2022/03/01/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%A4%8D%E4%B9%A0%E6%96%87%E6%A1%A3/">https://lexie1216.github.io/2022/03/01/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%A4%8D%E4%B9%A0%E6%96%87%E6%A1%A3/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://lexie1216.github.io" target="_blank">杀杀人上上网</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-full"><a href="/2022/03/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="深度学习复习"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">深度学习复习</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Lexie</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">5</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/lexie1216"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%A4%8D%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">模式识别复习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E7%BB%AA%E8%AE%BA"><span class="toc-number">1.1.</span> <span class="toc-text">一、绪论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB"><span class="toc-number">1.1.1.</span> <span class="toc-text">1.什么是模式识别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%8F%91%E5%B1%95%E7%AE%80%E5%8F%B2"><span class="toc-number">1.1.2.</span> <span class="toc-text">2.模式识别发展简史</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%BD%A2%E5%BC%8F%E5%8C%96"><span class="toc-number">1.1.3.</span> <span class="toc-text">3.模式识别形式化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E6%B5%81%E7%A8%8B"><span class="toc-number">1.1.4.</span> <span class="toc-text">4.模式识别系统流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E6%A8%A1%E5%BC%8F%E5%88%86%E7%B1%BB%E5%99%A8%E8%AE%BE%E8%AE%A1"><span class="toc-number">1.1.5.</span> <span class="toc-text">5.模式分类器设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E6%96%B9%E6%B3%95%E5%88%86%E7%B1%BB"><span class="toc-number">1.1.6.</span> <span class="toc-text">6.模式识别方法分类</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.2.</span> <span class="toc-text">二、贝叶斯决策(生成模型)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E8%A1%A8%E7%A4%BA"><span class="toc-number">1.2.1.</span> <span class="toc-text">1.分类问题的表示</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%9C%80%E5%B0%8F%E9%A3%8E%E9%99%A9%E5%86%B3%E7%AD%96%EF%BC%88%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E7%9A%84%E4%B8%80%E8%88%AC%E5%BD%A2%E5%BC%8F%EF%BC%89"><span class="toc-number">1.2.2.</span> <span class="toc-text">3.最小风险决策（贝叶斯决策的一般形式）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%9C%80%E5%B0%8F%E8%AF%AF%E5%B7%AE%E7%8E%87%E5%88%86%E7%B1%BB%EF%BC%88%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87%E5%86%B3%E7%AD%96%EF%BC%89"><span class="toc-number">1.2.3.</span> <span class="toc-text">3.最小误差率分类（最大后验概率决策）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E5%B8%A6%E6%8B%92%E8%AF%86%E7%9A%84%E5%86%B3%E7%AD%96%EF%BC%88c-1-classes"><span class="toc-number">1.2.4.</span> <span class="toc-text">3.5. 带拒识的决策（c+1 classes)</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E8%A7%84%E5%88%99"><span class="toc-number">1.2.4.0.0.1.</span> <span class="toc-text">决策规则</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0%E5%92%8C%E5%86%B3%E7%AD%96%E9%9D%A2"><span class="toc-number">1.2.5.</span> <span class="toc-text">4.判别函数和决策面</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-0-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96%E7%94%A8%E4%BA%8E%E6%A8%A1%E5%BC%8F%E5%88%86%E7%B1%BB"><span class="toc-number">1.2.6.</span> <span class="toc-text">5.0. 贝叶斯决策用于模式分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E9%AB%98%E6%96%AF%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6"><span class="toc-number">1.2.7.</span> <span class="toc-text">5.高斯概率密度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E9%AB%98%E6%96%AF%E5%AF%86%E5%BA%A6%E4%B8%8B%E7%9A%84%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0%EF%BC%88LDF-amp-QDF%EF%BC%89"><span class="toc-number">1.2.8.</span> <span class="toc-text">6.高斯密度下的判别函数（LDF&amp;QDF）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E5%88%86%E7%B1%BB%E9%94%99%E8%AF%AF%E7%8E%87"><span class="toc-number">1.2.9.</span> <span class="toc-text">7.分类错误率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E7%A6%BB%E6%95%A3%E5%8F%98%E9%87%8F%E7%9A%84%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%86%B3%E7%AD%96"><span class="toc-number">1.2.10.</span> <span class="toc-text">8.离散变量的贝叶斯决策</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-%E5%A4%8D%E5%90%88%E6%A8%A1%E5%BC%8F%E5%88%86%E7%B1%BB"><span class="toc-number">1.2.11.</span> <span class="toc-text">9.复合模式分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-%E5%BE%80%E5%B9%B4%E8%AF%95%E9%A2%98"><span class="toc-number">1.2.12.</span> <span class="toc-text">10.往年试题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1"><span class="toc-number">1.3.</span> <span class="toc-text">三、参数估计</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1"><span class="toc-number">1.3.1.</span> <span class="toc-text">1.最大似然估计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1"><span class="toc-number">1.3.2.</span> <span class="toc-text">2.贝叶斯估计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%92%8C%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1%E5%AF%B9%E6%AF%94"><span class="toc-number">1.3.3.</span> <span class="toc-text">*2.5最大似然估计和贝叶斯估计对比</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%89%B9%E5%BE%81%E7%BB%B4%E6%95%B0%E9%97%AE%E9%A2%98"><span class="toc-number">1.3.4.</span> <span class="toc-text">3.特征维数问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%9C%9F%E6%9C%9B%E6%9C%80%E5%A4%A7%E6%B3%95"><span class="toc-number">1.3.5.</span> <span class="toc-text">4.期望最大法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.3.6.</span> <span class="toc-text">5.隐马尔可夫模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E5%BE%80%E5%B9%B4%E8%AF%95%E9%A2%98"><span class="toc-number">1.3.7.</span> <span class="toc-text">6.往年试题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E9%9D%9E%E5%8F%82%E6%95%B0%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.</span> <span class="toc-text">四、非参数方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1"><span class="toc-number">1.4.1.</span> <span class="toc-text">1.密度估计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Parzen%E7%AA%97%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.2.</span> <span class="toc-text">2.Parzen窗方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-Parzen%E7%AA%97%E4%BC%B0%E8%AE%A1%E5%92%8Ck-%E8%BF%91%E9%82%BB%E4%BC%B0%E8%AE%A1%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.4.3.</span> <span class="toc-text">*2.5.Parzen窗估计和k-近邻估计的区别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-K%E8%BF%91%E9%82%BB%E4%BC%B0%E8%AE%A1"><span class="toc-number">1.4.4.</span> <span class="toc-text">3.K近邻估计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%9C%80%E8%BF%91%E9%82%BB%E8%A7%84%E5%88%99"><span class="toc-number">1.4.5.</span> <span class="toc-text">4.最近邻规则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F"><span class="toc-number">1.4.6.</span> <span class="toc-text">5.距离度量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Approximation-by-Series-Expansion"><span class="toc-number">1.4.7.</span> <span class="toc-text">6.Approximation by Series Expansion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E5%BE%80%E5%B9%B4%E8%AF%95%E9%A2%98"><span class="toc-number">1.4.8.</span> <span class="toc-text">7.往年试题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%94%E3%80%81%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0"><span class="toc-number">1.5.</span> <span class="toc-text">五、线性判别函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0%E4%B8%8E%E5%86%B3%E7%AD%96%E9%9D%A2"><span class="toc-number">1.5.1.</span> <span class="toc-text">1.线性判别函数与决策面</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%87%BD%E6%95%B0"><span class="toc-number">1.5.2.</span> <span class="toc-text">2.广义线性判别函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%84%9F%E7%9F%A5%E5%87%86%E5%88%99%E5%87%BD%E6%95%B0"><span class="toc-number">1.5.3.</span> <span class="toc-text">3.感知准则函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%9D%BE%E5%BC%9B%E6%96%B9%E6%B3%95"><span class="toc-number">1.5.4.</span> <span class="toc-text">4.松弛方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E6%9C%80%E5%B0%8F%E5%B9%B3%E6%96%B9%E8%AF%AF%E5%B7%AE%EF%BC%88MSE%EF%BC%89%E5%87%86%E5%88%99%E5%87%BD%E6%95%B0"><span class="toc-number">1.5.5.</span> <span class="toc-text">5.最小平方误差（MSE）准则函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Ho-Kashyap%E6%96%B9%E6%B3%95"><span class="toc-number">1.5.6.</span> <span class="toc-text">6.Ho-Kashyap方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E5%BE%80%E5%B9%B4%E8%AF%95%E9%A2%98-1"><span class="toc-number">1.5.7.</span> <span class="toc-text">7.往年试题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AD%E3%80%81%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.6.</span> <span class="toc-text">六、神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E5%85%83"><span class="toc-number">1.6.1.</span> <span class="toc-text">1.人工神经元</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%8B%93%E6%89%91%E7%BB%93%E6%9E%84"><span class="toc-number">1.6.2.</span> <span class="toc-text">2.拓扑结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%BD%91%E7%BB%9C%E8%AE%AD%E7%BB%83"><span class="toc-number">1.6.3.</span> <span class="toc-text">3.网络训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%8D%95%E5%B1%82%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88%E5%8D%95%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%89"><span class="toc-number">1.6.4.</span> <span class="toc-text">4.单层前馈神经网络（单层感知机）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8"><span class="toc-number">1.6.5.</span> <span class="toc-text">5.多层感知器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-BP%E7%AE%97%E6%B3%95"><span class="toc-number">1.6.6.</span> <span class="toc-text">6.BP算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E5%BE%84%E5%90%91%E5%9F%BA%E5%87%BD%E6%95%B0%E7%BD%91%E7%BB%9C"><span class="toc-number">1.6.7.</span> <span class="toc-text">7.径向基函数网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E5%8F%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.6.8.</span> <span class="toc-text">8.反馈神经网络</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-%E8%87%AA%E7%BB%84%E7%BB%87%E6%98%A0%E5%B0%84"><span class="toc-number">1.6.9.</span> <span class="toc-text">9.自组织映射</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9CCNN"><span class="toc-number">1.6.10.</span> <span class="toc-text">10.卷积神经网络CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">1.6.11.</span> <span class="toc-text">11.自编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-Recurrent-NN"><span class="toc-number">1.6.12.</span> <span class="toc-text">12.Recurrent NN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-LSTM"><span class="toc-number">1.6.13.</span> <span class="toc-text">13.LSTM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-%E5%BE%80%E5%B9%B4%E8%AF%95%E9%A2%98"><span class="toc-number">1.6.14.</span> <span class="toc-text">14.往年试题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%83%E3%80%81%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E4%B8%8E%E9%80%89%E6%8B%A9"><span class="toc-number">1.7.</span> <span class="toc-text">七、特征提取与选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96"><span class="toc-number">1.7.1.</span> <span class="toc-text">1.特征提取</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E7%89%B9%E5%BE%81%E5%8F%98%E6%8D%A2"><span class="toc-number">1.7.2.</span> <span class="toc-text">2.特征变换</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-number">1.7.3.</span> <span class="toc-text">3.特征选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%BE%80%E5%B9%B4%E8%AF%95%E9%A2%98"><span class="toc-number">1.7.4.</span> <span class="toc-text">4.往年试题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%AB%E3%80%81%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">1.8.</span> <span class="toc-text">八、模型选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%BC%95%E8%A8%80"><span class="toc-number">1.8.1.</span> <span class="toc-text">1.引言</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%E5%8E%9F%E5%88%99"><span class="toc-number">1.8.2.</span> <span class="toc-text">2.模型选择原则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7%E6%A0%87%E5%87%86"><span class="toc-number">1.8.3.</span> <span class="toc-text">3.模型评价标准</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%88%86%E7%B1%BB%E5%99%A8%E9%9B%86%E6%88%90"><span class="toc-number">1.8.4.</span> <span class="toc-text">4.分类器集成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E5%B1%82%E5%8F%A0%E6%B3%9B%E5%8C%96Stacked-Generalization"><span class="toc-number">1.8.5.</span> <span class="toc-text">5.层叠泛化Stacked Generalization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E6%A0%B7%E6%9C%AC%E8%A3%85%E8%A2%8BBagging"><span class="toc-number">1.8.6.</span> <span class="toc-text">6.样本装袋Bagging</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E9%9A%8F%E6%9C%BA%E5%AD%90%E7%A9%BA%E9%97%B4Random-Subspace"><span class="toc-number">1.8.7.</span> <span class="toc-text">7.随机子空间Random Subspace</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-Adaboost"><span class="toc-number">1.8.8.</span> <span class="toc-text">8.Adaboost</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-%E5%AF%B9Adaboost%E7%9A%84%E7%90%86%E8%AE%BA%E8%A7%A3%E9%87%8A"><span class="toc-number">1.8.9.</span> <span class="toc-text">9.对Adaboost的理论解释</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-%E5%9F%BA%E4%BA%8EAdaboost%E7%9A%84%E4%BA%BA%E8%84%B8%E6%A3%80%E6%B5%8B"><span class="toc-number">1.8.10.</span> <span class="toc-text">10.基于Adaboost的人脸检测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-%E5%BE%80%E5%B9%B4%E8%AF%95%E9%A2%98"><span class="toc-number">1.8.11.</span> <span class="toc-text">11.往年试题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B9%9D%E3%80%81%E8%81%9A%E7%B1%BB"><span class="toc-number">1.9.</span> <span class="toc-text">九、聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%BC%95%E8%A8%80-1"><span class="toc-number">1.9.1.</span> <span class="toc-text">1.引言</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E8%B7%9D%E7%A6%BB%E4%B8%8E%E7%9B%B8%E4%BC%BC%E6%80%A7%E5%BA%A6%E9%87%8F"><span class="toc-number">1.9.2.</span> <span class="toc-text">2.距离与相似性度量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-K-means"><span class="toc-number">1.9.3.</span> <span class="toc-text">3.K-means</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Gaussian-Mixture-Models"><span class="toc-number">1.9.4.</span> <span class="toc-text">4.Gaussian Mixture Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Hierachical-Clustring"><span class="toc-number">1.9.5.</span> <span class="toc-text">5.Hierachical Clustring</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Spectral-Clustering"><span class="toc-number">1.9.6.</span> <span class="toc-text">6.Spectral Clustering</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-Kernel-Clustering"><span class="toc-number">1.9.7.</span> <span class="toc-text">7.Kernel Clustering</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-Deep-Clustering"><span class="toc-number">1.9.8.</span> <span class="toc-text">8.Deep Clustering</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-%E5%BE%80%E5%B9%B4%E8%AF%95%E9%A2%98"><span class="toc-number">1.9.9.</span> <span class="toc-text">9.往年试题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%81%E3%80%81%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%B8%8E%E6%A0%B8%E6%96%B9%E6%B3%95"><span class="toc-number">1.10.</span> <span class="toc-text">十、支持向量机与核方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96"><span class="toc-number">1.10.1.</span> <span class="toc-text">1.结构风险最小化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-VC%E7%BB%B4"><span class="toc-number">1.10.2.</span> <span class="toc-text">2.VC维</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Hard-Margin-SVM"><span class="toc-number">1.10.3.</span> <span class="toc-text">3.Hard-Margin SVM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Soft-Margin-SVM"><span class="toc-number">1.10.4.</span> <span class="toc-text">4.Soft-Margin SVM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Dual-Problem"><span class="toc-number">1.10.5.</span> <span class="toc-text">5.Dual Problem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Kernel-Methods"><span class="toc-number">1.10.6.</span> <span class="toc-text">6.Kernel Methods</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">1.10.7.</span> <span class="toc-text">7.模型选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-%E5%BE%80%E5%B9%B4%E8%AF%95%E9%A2%98"><span class="toc-number">1.10.8.</span> <span class="toc-text">8.往年试题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2018%E5%B9%B4"><span class="toc-number">1.11.</span> <span class="toc-text">2018年</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2017%E5%B9%B4"><span class="toc-number">1.12.</span> <span class="toc-text">2017年</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2016%E5%B9%B4"><span class="toc-number">1.13.</span> <span class="toc-text">2016年</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%81%E4%B8%80%E3%80%81%E5%86%B3%E7%AD%96%E6%A0%91%E6%96%B9%E6%B3%95"><span class="toc-number">1.14.</span> <span class="toc-text">十一、决策树方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%AE%97%E6%B3%95"><span class="toc-number">1.14.1.</span> <span class="toc-text">1.信息增益算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-ID3"><span class="toc-number">1.14.2.</span> <span class="toc-text">2.ID3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-C4-5"><span class="toc-number">1.14.3.</span> <span class="toc-text">3.C4.5</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-CART"><span class="toc-number">1.14.4.</span> <span class="toc-text">4.CART</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-%E8%BF%87%E6%8B%9F%E5%90%88"><span class="toc-number">1.14.5.</span> <span class="toc-text">5.过拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">1.14.6.</span> <span class="toc-text">6.随机森林</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-%E5%BE%80%E5%B9%B4%E8%AF%95%E9%A2%98-2"><span class="toc-number">1.14.7.</span> <span class="toc-text">7.往年试题</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/04/20/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" title="强化学习复习">强化学习复习</a><time datetime="2022-04-19T16:00:00.000Z" title="发表于 2022-04-20 00:00:00">2022-04-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/03/01/SEIR%E6%A8%A1%E5%9E%8B/" title="SEIR模型">SEIR模型</a><time datetime="2022-02-28T16:00:00.000Z" title="发表于 2022-03-01 00:00:00">2022-03-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/03/01/%E9%AB%98%E7%BA%A7AI%E5%A4%8D%E4%B9%A0%E6%96%87%E6%A1%A3/" title="高级AI复习">高级AI复习</a><time datetime="2022-02-28T16:00:00.000Z" title="发表于 2022-03-01 00:00:00">2022-03-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/03/01/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" title="深度学习复习">深度学习复习</a><time datetime="2022-02-28T16:00:00.000Z" title="发表于 2022-03-01 00:00:00">2022-03-01</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/03/01/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB%E5%A4%8D%E4%B9%A0%E6%96%87%E6%A1%A3/" title="模式识别复习">模式识别复习</a><time datetime="2022-02-28T16:00:00.000Z" title="发表于 2022-03-01 00:00:00">2022-03-01</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Lexie</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'ilFxnBEYpPDWNvhHbDvo0Zum-gzGzoHsz',
      appKey: 'U8C4rtBHClosiEAhZbtdESn3',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>